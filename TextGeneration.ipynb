{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGeneration.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "15t---92VWE_49oSvmUwmiomEXG2pXmME",
      "authorship_tag": "ABX9TyN3gFJ+8c0bamVO9HRqpfPP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzizMosbah/TextGeneration/blob/main/TextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning a light version of GPT2 for Language Generation\n",
        "\n",
        "In this notebook we will be finetuning a state of the art language generation model to show if we can program an AI to have humour. \n",
        "\n",
        "Go straight to the last cells if you want to see it crack some jokes"
      ],
      "metadata": {
        "id": "T3dNHLOIZmzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the libraries"
      ],
      "metadata": {
        "id": "WmbzfLufaW8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx0rF4jqMhst"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "import random\n",
        "from transformers import pipeline\n",
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "TC8GREt4Mk3e"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the parameters of the model "
      ],
      "metadata": {
        "id": "YbJNZ5BZaaxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "train_path = \"/content/drive/MyDrive/Colab Notebooks/Data/jokes.txt\"\n",
        "output_dir=\"./gpt2-distil\" #The output directory where the model is saved\n",
        "num_train_epochs=5 # number of training epochs\n",
        "batch_size=32 # batch size\n",
        "eval_steps=400 # Number of update steps between two evaluations.\n",
        "save_steps=800 # after # steps model is saved\n",
        "warmup_steps=500 # number of warmup steps for learning rate scheduler\n",
        "lr=5e-5"
      ],
      "metadata": {
        "id": "5e9_7qiYMqdR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and pre-processing the data "
      ],
      "metadata": {
        "id": "B5mI7o1eakr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "def load_dataset(train_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset, data_collator"
      ],
      "metadata": {
        "id": "M1sCm3rYNh0r"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, data_collator = load_dataset(train_path, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2prkhf0bNpUj",
        "outputId": "2b146776-7e6a-4a13-96d1-77d38fa33b1f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True, \n",
        "    num_train_epochs=num_train_epochs, \n",
        "    per_device_train_batch_size=batch_size, \n",
        "    per_device_eval_batch_size= batch_size,  \n",
        "    eval_steps = eval_steps, \n",
        "    save_steps= save_steps,  \n",
        "    warmup_steps= warmup_steps,\n",
        "    prediction_loss_only=True,\n",
        "    logging_steps = 100,\n",
        "    )\n",
        "\n",
        "optimizer = Adafactor(\n",
        "    model.parameters(),\n",
        "    lr=5e-5,\n",
        "    eps=(1e-30, 5e-5),\n",
        "    clip_threshold=1.0,\n",
        "    decay_rate=-0.8,\n",
        "    beta1=None,\n",
        "    weight_decay=0.0,\n",
        "    relative_step=False,\n",
        "    scale_parameter=False,\n",
        "    warmup_init=False,\n",
        ")\n",
        "\n",
        "lr_scheduler = AdafactorSchedule(optimizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers=(optimizer, lr_scheduler),\n",
        "    \n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Lm8jP8b1N7DD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0bc58f-3c28-4048-dc15-3a8be63e2c63"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model and building the jokes pipeline"
      ],
      "metadata": {
        "id": "-BcN6_1aav9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "fxiOPim6N9v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cracks_a_joke(input):\n",
        "  input_ids = tokenizer.encode(input, return_tensors='pt')\n",
        "  greedy_output = model.generate(input_ids, max_length=50)\n",
        "  print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "vbfeP0ckVxXx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's hear it ! ðŸ˜†"
      ],
      "metadata": {
        "id": "jjzaKLL3a0kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cracks_a_joke('A priest enters a pub')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoCKcEM5WALE",
        "outputId": "12c6b3ee-8b9d-4121-c88e-6dff654e531c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A priest enters a pub. The priest says, \"Hey, what's this, some kind of joke?\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cracks_a_joke('What is')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrfbBYBLWMwz",
        "outputId": "09f63ca0-7327-4946-cfc6-10e35b990e76"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the difference between a garbanzo bean and a chickpea? I wouldn't pay $200 to have a garbanzo bean on my face. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cracks_a_joke('What do you call')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BY4HygpWVkJ",
        "outputId": "0bae26f4-dd03-4fc8-b6c1-b35782a61813"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you call a dog with no legs? A shih tzu. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cracks_a_joke('What do you call')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DO2hvADWlyo",
        "outputId": "86037959-b691-4f5e-8990-f77cfb14118a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you call a group of people who are in a relationship with each other? A group of people who are in a relationship with each other. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cracks_a_joke('When is')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pQtvy_KWqbq",
        "outputId": "45c6c65d-fe83-4da3-c997-b6a3ee8124d6"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When is the best time to go to the dentist? Tooth hurty. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cracks_a_joke('knock')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CB5imXPXWjG",
        "outputId": "2e3aa0c6-f9f1-492a-f1cb-3a5b0488b078"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knock Knock knock knock. Who's there? The guy who? The guy who? The guy who? The guy who? The guy who? The guy who? The guy who? The guy who? The guy who? The guy who?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cracks_a_joke(\"Who's\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4N_jlNvX0qy",
        "outputId": "a41ca608-cb14-4e5a-c80b-56653443a88a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who's there? The guy who's in the bathroom. \n"
          ]
        }
      ]
    }
  ]
}